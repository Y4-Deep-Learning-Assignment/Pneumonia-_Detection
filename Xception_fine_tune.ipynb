{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Y4-Deep-Learning-Assignment/Pneumonia-_Detection/blob/Xception/Xception_fine_tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Xception Transfer Learning for Pneumonia Detection"
      ],
      "metadata": {
        "id": "QN8HWBVKM6_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIC786jUMows",
        "outputId": "e4b3a234-b873-40e3-c43d-b0911a11a0f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and import required libraries"
      ],
      "metadata": {
        "id": "ywBiptbhObNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3_3vJoYyOeLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow keras numpy pandas matplotlib seaborn scikit-learn -q\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Keras version:\", keras.__version__)\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"\\nGPU Available:\", tf.test.is_gpu_available())\n",
        "if tf.test.is_gpu_available():\n",
        "    print(\"GPU Device:\", tf.test.gpu_device_name())"
      ],
      "metadata": {
        "id": "0k58bTkTOqlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Dataset"
      ],
      "metadata": {
        "id": "XqqM_bilPC-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/DL Assignment/archive.zip\"\n",
        "\n",
        "shutil.copy(zip_path, \"/content/\")\n",
        "\n",
        "# Extract the dataset\n",
        "print(\"Extracting dataset...\")\n",
        "with zipfile.ZipFile(\"/content/archive.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')\n",
        "\n",
        "print(\"Dataset structure:\")\n",
        "!find /content/chest_xray -type d -print\n"
      ],
      "metadata": {
        "id": "7M5Kz7d_PFC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore the Dataset"
      ],
      "metadata": {
        "id": "1wNW2co8PLxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/chest_xray'\n",
        "train_path = os.path.join(dataset_path, 'train')\n",
        "test_path = os.path.join(dataset_path, 'test')\n",
        "val_path = os.path.join(dataset_path, 'val')\n",
        "\n",
        "def explore_dataset():\n",
        "    print(\"Dataset Exploration:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for split in ['train', 'test', 'val']:\n",
        "        split_path = os.path.join(dataset_path, split)\n",
        "        normal_path = os.path.join(split_path, 'NORMAL')\n",
        "        pneumonia_path = os.path.join(split_path, 'PNEUMONIA')\n",
        "\n",
        "        normal_count = len(os.listdir(normal_path))\n",
        "        pneumonia_count = len(os.listdir(pneumonia_path))\n",
        "        total_count = normal_count + pneumonia_count\n",
        "\n",
        "        print(f\"\\n{split.upper()} SET:\")\n",
        "        print(f\"  Normal images: {normal_count}\")\n",
        "        print(f\"  Pneumonia images: {pneumonia_count}\")\n",
        "        print(f\"  Total images: {total_count}\")\n",
        "        print(f\"  Pneumonia ratio: {pneumonia_count/total_count:.2%}\")\n",
        "\n",
        "explore_dataset()"
      ],
      "metadata": {
        "id": "rP_okKwAPOVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize sample images\n",
        "def visualize_samples():\n",
        "    \"\"\"Display sample images from both classes\"\"\"\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
        "\n",
        "    # Normal samples\n",
        "    normal_path = os.path.join(train_path, 'NORMAL')\n",
        "    normal_images = [f for f in os.listdir(normal_path) if f.endswith(('.jpeg', '.jpg', '.png'))][:4]\n",
        "\n",
        "    for i, img_name in enumerate(normal_images):\n",
        "        img_path = os.path.join(normal_path, img_name)\n",
        "        img = plt.imread(img_path)\n",
        "        axes[0, i].imshow(img, cmap='gray')\n",
        "        axes[0, i].set_title(f'Normal\\n{img_name}', fontsize=10)\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "    # Pneumonia samples\n",
        "    pneumonia_path = os.path.join(train_path, 'PNEUMONIA')\n",
        "    pneumonia_images = [f for f in os.listdir(pneumonia_path) if f.endswith(('.jpeg', '.jpg', '.png'))][:4]\n",
        "\n",
        "    for i, img_name in enumerate(pneumonia_images):\n",
        "        img_path = os.path.join(pneumonia_path, img_name)\n",
        "        img = plt.imread(img_path)\n",
        "        axes[1, i].imshow(img, cmap='gray')\n",
        "        axes[1, i].set_title(f'Pneumonia\\n{img_name}', fontsize=10)\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.suptitle('Sample Chest X-Ray Images - Xception Model', fontsize=16, y=0.95)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_samples()"
      ],
      "metadata": {
        "id": "erkIHopQPZhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing and Augmentation"
      ],
      "metadata": {
        "id": "TQCzK8SzP8Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_HEIGHT = 299  # Xception expects 299x299 images\n",
        "IMG_WIDTH = 299\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "print(\"Setting up data generators for Xception...\")\n",
        "\n",
        "# Enhanced data augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,  # Increased from 15\n",
        "    width_shift_range=0.15,  # Increased from 0.1\n",
        "    height_shift_range=0.15,  # Increased from 0.1\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.25,  # Increased from 0.2\n",
        "    brightness_range=[0.8, 1.2],  # Wider range\n",
        "    shear_range=0.15,  # Added shear\n",
        "    fill_mode='constant',\n",
        "    cval=0\n",
        ")\n",
        "\n",
        "# Only rescaling for validation and test\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "-Lu1mPrOP-oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create proper validation set by splitting training data\n",
        "import tempfile\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def create_proper_validation_split(validation_size=0.15):\n",
        "    \"\"\"\n",
        "    Split training data to create a proper validation set\n",
        "    \"\"\"\n",
        "    print(\"Creating proper validation split...\")\n",
        "\n",
        "    # Create temporary directories\n",
        "    base_temp_dir = tempfile.mkdtemp()\n",
        "    new_train_dir = os.path.join(base_temp_dir, 'train')\n",
        "    new_val_dir = os.path.join(base_temp_dir, 'val')\n",
        "\n",
        "    # Create subdirectories\n",
        "    for split_dir in [new_train_dir, new_val_dir]:\n",
        "        os.makedirs(os.path.join(split_dir, 'NORMAL'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(split_dir, 'PNEUMONIA'), exist_ok=True)\n",
        "\n",
        "    # Split each class\n",
        "    for class_name in ['NORMAL', 'PNEUMONIA']:\n",
        "        source_dir = os.path.join(train_path, class_name)\n",
        "        images = [f for f in os.listdir(source_dir) if f.endswith(('.jpeg', '.jpg', '.png'))]\n",
        "\n",
        "        # Split images\n",
        "        train_imgs, val_imgs = train_test_split(\n",
        "            images,\n",
        "            test_size=validation_size,\n",
        "            random_state=42,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        # Copy images to new directories\n",
        "        for img in train_imgs:\n",
        "            shutil.copy2(\n",
        "                os.path.join(source_dir, img),\n",
        "                os.path.join(new_train_dir, class_name, img)\n",
        "            )\n",
        "        for img in val_imgs:\n",
        "            shutil.copy2(\n",
        "                os.path.join(source_dir, img),\n",
        "                os.path.join(new_val_dir, class_name, img)\n",
        "            )\n",
        "\n",
        "        print(f\"  {class_name}: {len(train_imgs)} train, {len(val_imgs)} validation\")\n",
        "\n",
        "    return new_train_dir, new_val_dir, base_temp_dir\n",
        "\n",
        "# Create proper train/validation split\n",
        "new_train_dir, new_val_dir, temp_dir = create_proper_validation_split(validation_size=0.15)\n",
        "\n",
        "# Create IMPROVED data generators with proper validation set\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    new_train_dir,  # Use new training directory\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    color_mode='rgb',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "validation_generator = val_test_datagen.flow_from_directory(\n",
        "    new_val_dir,  # Use new validation directory\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    color_mode='rgb',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Keep original test set unchanged\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    test_path,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    color_mode='rgb',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Display class information\n",
        "class_indices = train_generator.class_indices\n",
        "class_names = list(class_indices.keys())\n",
        "\n",
        "print(f\"\\nIMPROVED Data Generators Created Successfully!\")\n",
        "print(f\"Class indices: {class_indices}\")\n",
        "print(f\"Training samples: {train_generator.samples}\")\n",
        "print(f\"Validation samples: {validation_generator.samples}\")  # Now proper size!\n",
        "print(f\"Test samples: {test_generator.samples}\")\n",
        "print(f\"Class names: {class_names}\")\n",
        "\n",
        "# Calculate class weights based on NEW training data distribution\n",
        "normal_count = len(os.listdir(os.path.join(new_train_dir, 'NORMAL')))\n",
        "pneumonia_count = len(os.listdir(os.path.join(new_train_dir, 'PNEUMONIA')))\n",
        "total = normal_count + pneumonia_count\n",
        "\n",
        "weight_for_0 = total / (2 * normal_count)    # Weight for NORMAL\n",
        "weight_for_1 = total / (2 * pneumonia_count)  # Weight for PNEUMONIA\n",
        "\n",
        "class_weights = {0: weight_for_0, 1: weight_for_1}\n",
        "print(f\"Class weights: {class_weights}\")\n"
      ],
      "metadata": {
        "id": "im4bNzo0QEcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Xception Transfer Learning Model"
      ],
      "metadata": {
        "id": "URTCDDvDQhNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_xception_model():\n",
        "    \"\"\"\n",
        "    Create Xception transfer learning model for pneumonia detection\n",
        "    Xception is known for its depthwise separable convolutions\n",
        "    \"\"\"\n",
        "    print(\"Building Xception model...\")\n",
        "\n",
        "    # Load pre-trained Xception\n",
        "    base_model = Xception(\n",
        "        weights='imagenet',       # Pre-trained on ImageNet\n",
        "        include_top=False,        # Exclude the final classification layers\n",
        "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)\n",
        "    )\n",
        "\n",
        "    # Freeze the base model layers initially\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Create the complete model\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        GlobalAveragePooling2D(),\n",
        "        BatchNormalization(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        BatchNormalization(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        BatchNormalization(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1, activation='sigmoid')  # Binary classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "xception_model = create_xception_model()\n",
        "\n",
        "# Display model summary\n",
        "print(\"\\nXception Model Summary:\")\n",
        "xception_model.summary()"
      ],
      "metadata": {
        "id": "jowIJ7OPQt1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Callbacks for Training"
      ],
      "metadata": {
        "id": "YF9GPXwkQ8VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_callbacks():\n",
        "    \"\"\"\n",
        "    Set up callbacks for efficient training\n",
        "    \"\"\"\n",
        "    # Early stopping to prevent overfitting\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "        mode='max'\n",
        "    )\n",
        "\n",
        "    # Reduce learning rate when validation loss plateaus\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Save the best model\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        'best_xception_pneumonia_model.h5',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return [early_stop, reduce_lr, checkpoint]\n",
        "\n",
        "callbacks = setup_callbacks()\n",
        "print(\"Callbacks setup complete!\")"
      ],
      "metadata": {
        "id": "9cxoZEoSQ-v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model"
      ],
      "metadata": {
        "id": "1M7xCtsjRWuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting Xception model training...\")\n",
        "EPOCHS = 30\n",
        "\n",
        "# Calculate steps per epoch\n",
        "train_steps = train_generator.samples // BATCH_SIZE\n",
        "val_steps = validation_generator.samples // BATCH_SIZE\n",
        "\n",
        "print(f\"Training steps per epoch: {train_steps}\")\n",
        "print(f\"Validation steps per epoch: {val_steps}\")\n",
        "print(f\"Maximum epochs: {EPOCHS}\")\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "history = xception_model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_steps,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=val_steps,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights,  # Important for handling imbalance\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nXception training completed in {training_time/60:.2f} minutes!\")"
      ],
      "metadata": {
        "id": "hknP1AhCRYm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Model"
      ],
      "metadata": {
        "id": "WOg20RTrZUdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_xception_model_safe(model, test_generator):\n",
        "    \"\"\"\n",
        "    Ultra-safe evaluation that handles any metric naming issues\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating Xception model on test set...\")\n",
        "\n",
        "    # Get the actual metric names\n",
        "    print(\"Available metrics:\", model.metrics_names)\n",
        "\n",
        "    # Evaluate and get results as dictionary (TensorFlow 2.4+)\n",
        "    try:\n",
        "        test_results = model.evaluate(test_generator, verbose=1, return_dict=True)\n",
        "        print(\"Results as dictionary:\", test_results)\n",
        "    except:\n",
        "        # Fallback for older TensorFlow versions\n",
        "        test_results = model.evaluate(test_generator, verbose=1)\n",
        "        metrics_dict = dict(zip(model.metrics_names, test_results))\n",
        "        print(\"Results as list:\", test_results)\n",
        "        print(\"Metrics dictionary:\", metrics_dict)\n",
        "        test_results = metrics_dict\n",
        "\n",
        "    print(f\"\\nXception Test Results:\")\n",
        "\n",
        "    # Extract all available metrics\n",
        "    for metric_name, value in test_results.items():\n",
        "        print(f\"{metric_name}: {value:.4f}\")\n",
        "\n",
        "    # Calculate F1-score from available metrics\n",
        "    precision = test_results.get('precision', 0.0)\n",
        "    recall = test_results.get('recall', 0.0)\n",
        "\n",
        "    if precision + recall > 0:\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "        print(f\"F1-Score: {f1_score:.4f}\")\n",
        "\n",
        "    # Make predictions\n",
        "    print(\"\\nMaking predictions...\")\n",
        "    test_generator.reset()\n",
        "    predictions = model.predict(test_generator, verbose=1)\n",
        "    predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "    # True labels\n",
        "    true_classes = test_generator.classes\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_classes, predicted_classes,\n",
        "                              target_names=['NORMAL', 'PNEUMONIA'], digits=4))\n",
        "\n",
        "    return test_results, predictions, predicted_classes\n",
        "\n",
        "# Use the safe evaluation function\n",
        "test_metrics, predictions, predicted_classes = evaluate_xception_model_safe(xception_model, test_generator)"
      ],
      "metadata": {
        "id": "ZlNxfdc7Z10L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Results"
      ],
      "metadata": {
        "id": "e_1IcgFhashE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history_simple(history):\n",
        "    \"\"\"\n",
        "    Plot comprehensive training history without fine-tuning\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "    # Accuracy\n",
        "    axes[0, 0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, color='blue')\n",
        "    axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, color='red')\n",
        "    axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Loss\n",
        "    axes[0, 1].plot(history.history['loss'], label='Training Loss', linewidth=2, color='blue')\n",
        "    axes[0, 1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2, color='red')\n",
        "    axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Precision\n",
        "    if 'precision' in history.history:\n",
        "        axes[0, 2].plot(history.history['precision'], label='Training Precision', linewidth=2, color='blue')\n",
        "        axes[0, 2].plot(history.history['val_precision'], label='Validation Precision', linewidth=2, color='red')\n",
        "        axes[0, 2].set_title('Model Precision', fontsize=14, fontweight='bold')\n",
        "        axes[0, 2].set_xlabel('Epoch')\n",
        "        axes[0, 2].set_ylabel('Precision')\n",
        "        axes[0, 2].legend()\n",
        "        axes[0, 2].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        axes[0, 2].text(0.5, 0.5, 'Precision data\\nnot available',\n",
        "                       horizontalalignment='center', verticalalignment='center',\n",
        "                       transform=axes[0, 2].transAxes, fontsize=12)\n",
        "        axes[0, 2].set_title('Model Precision', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Recall\n",
        "    if 'recall' in history.history:\n",
        "        axes[1, 0].plot(history.history['recall'], label='Training Recall', linewidth=2, color='blue')\n",
        "        axes[1, 0].plot(history.history['val_recall'], label='Validation Recall', linewidth=2, color='red')\n",
        "        axes[1, 0].set_title('Model Recall', fontsize=14, fontweight='bold')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('Recall')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        axes[1, 0].text(0.5, 0.5, 'Recall data\\nnot available',\n",
        "                       horizontalalignment='center', verticalalignment='center',\n",
        "                       transform=axes[1, 0].transAxes, fontsize=12)\n",
        "        axes[1, 0].set_title('Model Recall', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # AUC\n",
        "    if 'auc' in history.history:\n",
        "        axes[1, 1].plot(history.history['auc'], label='Training AUC', linewidth=2, color='blue')\n",
        "        axes[1, 1].plot(history.history['val_auc'], label='Validation AUC', linewidth=2, color='red')\n",
        "        axes[1, 1].set_title('Model AUC', fontsize=14, fontweight='bold')\n",
        "        axes[1, 1].set_xlabel('Epoch')\n",
        "        axes[1, 1].set_ylabel('AUC')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        axes[1, 1].text(0.5, 0.5, 'AUC data\\nnot available',\n",
        "                       horizontalalignment='center', verticalalignment='center',\n",
        "                       transform=axes[1, 1].transAxes, fontsize=12)\n",
        "        axes[1, 1].set_title('Model AUC', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Learning Rate\n",
        "    if 'lr' in history.history:\n",
        "        axes[1, 2].plot(history.history['lr'], label='Learning Rate', linewidth=2, color='purple')\n",
        "        axes[1, 2].set_title('Learning Rate', fontsize=14, fontweight='bold')\n",
        "        axes[1, 2].set_xlabel('Epoch')\n",
        "        axes[1, 2].set_ylabel('Learning Rate')\n",
        "        axes[1, 2].set_yscale('log')\n",
        "        axes[1, 2].legend()\n",
        "        axes[1, 2].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        # Show training summary instead\n",
        "        final_train_acc = history.history['accuracy'][-1]\n",
        "        final_val_acc = history.history['val_accuracy'][-1]\n",
        "        final_train_loss = history.history['loss'][-1]\n",
        "        final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "        summary_text = f\"Final Training Accuracy: {final_train_acc:.4f}\\nFinal Validation Accuracy: {final_val_acc:.4f}\\nFinal Training Loss: {final_train_loss:.4f}\\nFinal Validation Loss: {final_val_loss:.4f}\"\n",
        "\n",
        "        axes[1, 2].text(0.5, 0.5, summary_text,\n",
        "                       horizontalalignment='center', verticalalignment='center',\n",
        "                       transform=axes[1, 2].transAxes, fontsize=11,\n",
        "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
        "        axes[1, 2].set_title('Training Summary', fontsize=14, fontweight='bold')\n",
        "        axes[1, 2].set_xticks([])\n",
        "        axes[1, 2].set_yticks([])\n",
        "\n",
        "    plt.suptitle('Xception Training History - Pneumonia Detection', fontsize=16, fontweight='bold', y=0.98)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the training history\n",
        "plot_training_history_simple(history)"
      ],
      "metadata": {
        "id": "jhPy81Osbdt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrix\n",
        "def plot_confusion_matrix_detailed(test_generator, predicted_classes):\n",
        "    \"\"\"\n",
        "    Plot detailed confusion matrix with percentages\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(test_generator.classes, predicted_classes)\n",
        "\n",
        "    # Calculate percentages\n",
        "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Plot absolute values\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
        "                xticklabels=['NORMAL', 'PNEUMONIA'],\n",
        "                yticklabels=['NORMAL', 'PNEUMONIA'],\n",
        "                annot_kws={\"size\": 14})\n",
        "    ax1.set_title('Confusion Matrix (Absolute Values)\\nXception Model',\n",
        "                  fontsize=14, fontweight='bold', pad=20)\n",
        "    ax1.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Plot percentages\n",
        "    sns.heatmap(cm_percentage, annot=True, fmt='.2f', cmap='Greens', ax=ax2,\n",
        "                xticklabels=['NORMAL', 'PNEUMONIA'],\n",
        "                yticklabels=['NORMAL', 'PNEUMONIA'],\n",
        "                annot_kws={\"size\": 12})\n",
        "    ax2.set_title('Confusion Matrix (Percentages)\\nXception Model',\n",
        "                  fontsize=14, fontweight='bold', pad=20)\n",
        "    ax2.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print numerical analysis\n",
        "    print(\"Confusion Matrix Analysis:\")\n",
        "    print(f\"True Negatives (Normal correctly classified): {cm[0,0]} ({cm_percentage[0,0]:.1f}%)\")\n",
        "    print(f\"False Positives (Normal misclassified as Pneumonia): {cm[0,1]} ({cm_percentage[0,1]:.1f}%)\")\n",
        "    print(f\"False Negatives (Pneumonia misclassified as Normal): {cm[1,0]} ({cm_percentage[1,0]:.1f}%)\")\n",
        "    print(f\"True Positives (Pneumonia correctly classified): {cm[1,1]} ({cm_percentage[1,1]:.1f}%)\")\n",
        "\n",
        "plot_confusion_matrix_detailed(test_generator, predicted_classes)"
      ],
      "metadata": {
        "id": "dyCSCPxSbjp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC Curve\n",
        "def plot_roc_curve(test_generator, predictions):\n",
        "    \"\"\"\n",
        "    Plot ROC curve for model performance\n",
        "    \"\"\"\n",
        "    fpr, tpr, thresholds = roc_curve(test_generator.classes, predictions)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title('Xception - Receiver Operating Characteristic (ROC) Curve', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    return roc_auc\n",
        "\n",
        "roc_auc = plot_roc_curve(test_generator, predictions)\n",
        "print(f\"ROC AUC Score: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "id": "aDwtHuaqb79B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Predictions Visualization"
      ],
      "metadata": {
        "id": "ZAikJAm7cA_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_sample_predictions(model, test_generator, num_samples=8):\n",
        "    \"\"\"\n",
        "    Display sample predictions with true and predicted labels\n",
        "    \"\"\"\n",
        "    # Get a batch of test data\n",
        "    test_generator.reset()\n",
        "    x_batch, y_batch = next(test_generator)\n",
        "\n",
        "    # Make predictions for this batch\n",
        "    predictions = model.predict(x_batch[:num_samples], verbose=0)\n",
        "    predicted_labels = (predictions > 0.5).astype(int).flatten()\n",
        "    true_labels = y_batch[:num_samples].astype(int)\n",
        "\n",
        "    # Class names\n",
        "    class_names_dict = {0: 'NORMAL', 1: 'PNEUMONIA'}\n",
        "\n",
        "    # Plot samples\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        axes[i].imshow(x_batch[i])\n",
        "        axes[i].axis('off')\n",
        "\n",
        "        true_class = class_names_dict[true_labels[i]]\n",
        "        pred_class = class_names_dict[predicted_labels[i]]\n",
        "        confidence = predictions[i][0]\n",
        "\n",
        "        # Color code: green for correct, red for incorrect\n",
        "        color = 'green' if true_labels[i] == predicted_labels[i] else 'red'\n",
        "\n",
        "        title = f'True: {true_class}\\nPred: {pred_class}\\nConf: {confidence:.3f}'\n",
        "        axes[i].set_title(title, color=color, fontsize=11, fontweight='bold', pad=10)\n",
        "\n",
        "    plt.suptitle('Xception - Sample Predictions on Test Set',\n",
        "                 fontsize=16, fontweight='bold', y=0.98)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "display_sample_predictions(xception_model, test_generator)"
      ],
      "metadata": {
        "id": "kLPwQKn0cDXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the Model and Results"
      ],
      "metadata": {
        "id": "_Oz1aiE5cpbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_xception_results_simple(model, history, test_metrics):\n",
        "    \"\"\"\n",
        "    Save model, training history, and metrics without fine-tuning\n",
        "    \"\"\"\n",
        "    print(\"Saving Xception results...\")\n",
        "\n",
        "    # Save the model\n",
        "    model.save('xception_pneumonia_detection.h5')\n",
        "    print(\"Model saved as 'xception_pneumonia_detection.h5'\")\n",
        "\n",
        "    # Save training history to CSV\n",
        "    history_df = pd.DataFrame(history.history)\n",
        "    history_df['epoch'] = range(1, len(history_df) + 1)\n",
        "    history_df.to_csv('xception_training_history.csv', index=False)\n",
        "    print(\"Training history saved as 'xception_training_history.csv'\")\n",
        "\n",
        "    # Save test metrics\n",
        "    metrics_df = pd.DataFrame([test_metrics])\n",
        "    metrics_df.to_csv('xception_test_metrics.csv', index=False)\n",
        "    print(\"Test metrics saved as 'xception_test_metrics.csv'\")\n",
        "\n",
        "    # Create performance summary\n",
        "    summary = {\n",
        "        'final_val_accuracy': history.history['val_accuracy'][-1],\n",
        "        'final_val_loss': history.history['val_loss'][-1],\n",
        "        'test_accuracy': test_metrics['accuracy'],\n",
        "        'test_precision': test_metrics['precision'],\n",
        "        'test_recall': test_metrics['recall'],\n",
        "        'test_auc': test_metrics['auc'],\n",
        "        'training_time_minutes': training_time / 60,\n",
        "        'model_parameters': model.count_params()\n",
        "    }\n",
        "\n",
        "    summary_df = pd.DataFrame([summary])\n",
        "    summary_df.to_csv('xception_performance_summary.csv', index=False)\n",
        "    print(\"Performance summary saved as 'xception_performance_summary.csv'\")\n",
        "\n",
        "# Save the results without fine-tuning data\n",
        "save_xception_results_simple(xception_model, history, test_metrics)"
      ],
      "metadata": {
        "id": "6YqWF_zCcrCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Summary"
      ],
      "metadata": {
        "id": "w_1wkqYIeicl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 14: Display final performance summary\n",
        "def display_xception_summary():\n",
        "    \"\"\"\n",
        "    Display comprehensive project summary for Xception\n",
        "    \"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"XCEPTION PNEUMONIA DETECTION - PROJECT SUMMARY\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(f\"\\nDATASET STATISTICS:\")\n",
        "    print(f\"   Training samples: {train_generator.samples}\")\n",
        "    print(f\"   Validation samples: {validation_generator.samples}\")\n",
        "    print(f\"   Test samples: {test_generator.samples}\")\n",
        "    print(f\"   Classes: {class_names}\")\n",
        "    print(f\"   Class weights: {class_weights}\")\n",
        "\n",
        "    print(f\"\\nMODEL ARCHITECTURE:\")\n",
        "    print(f\"   Base Model: Xception (pre-trained on ImageNet)\")\n",
        "    print(f\"   Input Size: {IMG_HEIGHT}x{IMG_WIDTH}x3\")\n",
        "    print(f\"   Trainable Parameters: {xception_model.trainable_weights.__len__()} layers\")\n",
        "    print(f\"   Total Parameters: {xception_model.count_params():,}\")\n",
        "    print(f\"   Key Feature: Depthwise Separable Convolutions\")\n",
        "\n",
        "    print(f\"\\nTRAINING PERFORMANCE:\")\n",
        "    print(f\"   Training Time: {training_time/60:.2f} minutes\")\n",
        "    print(f\"   Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
        "    print(f\"   Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
        "\n",
        "    print(f\"\\nTEST PERFORMANCE:\")\n",
        "    print(f\"   Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "    print(f\"   Test Precision: {test_metrics['precision']:.4f}\")\n",
        "    print(f\"   Test Recall: {test_metrics['recall']:.4f}\")\n",
        "    print(f\"   Test AUC: {test_metrics['auc']:.4f}\")\n",
        "    print(f\"   ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    f1 = 2 * (test_metrics['precision'] * test_metrics['recall']) / (test_metrics['precision'] + test_metrics['recall'])\n",
        "    print(f\"   Test F1-Score: {f1:.4f}\")\n",
        "\n",
        "    print(f\"\\nSAVED FILES:\")\n",
        "    print(\"   1. xception_pneumonia_detection.h5 - Trained model\")\n",
        "    print(\"   2. xception_training_history.csv - Training metrics history\")\n",
        "    print(\"   3. xception_test_metrics.csv - Detailed test metrics\")\n",
        "    print(\"   4. xception_performance_summary.csv - Performance summary\")\n",
        "\n",
        "display_xception_summary()"
      ],
      "metadata": {
        "id": "JpLnEuuHekUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning section"
      ],
      "metadata": {
        "id": "PVGWnzubDeTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning start"
      ],
      "metadata": {
        "id": "0DI2vwLSD0uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning section\n",
        "def fine_tune_model(model_to_tune, base_layers_unfreeze=30):\n",
        "    \"\"\"\n",
        "    Unfreeze the top layers of the base model for fine-tuning\n",
        "    \"\"\"\n",
        "    print(\"Inside fine_tune_model function...\")\n",
        "    if model_to_tune is None:\n",
        "        print(\"Error: Model passed to fine_tune_model is None.\")\n",
        "        return None\n",
        "\n",
        "    # Unfreeze the top N layers of the base model\n",
        "    model_to_tune.trainable = True\n",
        "    print(f\"Initial trainable status of model_to_tune: {model_to_tune.trainable}\")\n",
        "\n",
        "    # Freeze the bottom layers, unfreeze the top layers\n",
        "    # Check if the model has a base model as the first layer\n",
        "    if isinstance(model_to_tune.layers[0], tf.keras.Model):\n",
        "        print(\"First layer is a Keras Model (assuming base model).\")\n",
        "        base_model_layers = model_to_tune.layers[0].layers\n",
        "        print(f\"Total layers in base model: {len(base_model_layers)}\")\n",
        "        # Freeze all layers in the base model first\n",
        "        for layer in base_model_layers:\n",
        "            layer.trainable = False\n",
        "        print(\"All base model layers initially frozen.\")\n",
        "        # Unfreeze the top layers of the base model\n",
        "        unfrozen_count = 0\n",
        "        for layer in base_model_layers[-base_layers_unfreeze:]:\n",
        "            layer.trainable = True\n",
        "            unfrozen_count += 1\n",
        "        print(f\"Unfrozen top {unfrozen_count} layers of the base model for fine-tuning\")\n",
        "    else:\n",
        "        print(\"First layer is not a Keras Model. Unfreezing layers in the sequential model directly.\")\n",
        "        # If no explicit base model layer, unfreeze the last 'base_layers_unfreeze' layers of the sequential model\n",
        "        total_layers = len(model_to_tune.layers)\n",
        "        layers_to_freeze = max(0, total_layers - base_layers_unfreeze)\n",
        "        for layer in model_to_tune.layers[:layers_to_freeze]:\n",
        "            layer.trainable = False\n",
        "        for layer in model_to_tune.layers[layers_to_freeze:]:\n",
        "             layer.trainable = True\n",
        "        print(f\"Unfrozen top {total_layers - layers_to_freeze} layers of the sequential model for fine-tuning\")\n",
        "\n",
        "\n",
        "    # Recompile with a lower learning rate\n",
        "    print(\"Recompiling model with lower learning rate...\")\n",
        "    model_to_tune.compile(\n",
        "        optimizer=Adam(learning_rate=1e-5),  # Lower learning rate for fine-tuning\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', 'precision', 'recall']\n",
        "    )\n",
        "\n",
        "    print(f\"Trainable layers after unfreezing and recompile: {sum([layer.trainable for layer in model_to_tune.layers])}\")\n",
        "    print(f\"Total layers: {len(model_to_tune.layers)}\")\n",
        "    print(\"fine_tune_model function finished.\")\n",
        "\n",
        "    return model_to_tune\n",
        "\n",
        "# Fine-tune the model\n",
        "print(\"Starting fine-tuning...\")\n",
        "# Pass the created xception_model to the fine-tuning function\n",
        "# Ensure xception_model is defined and not None from previous steps\n",
        "if 'xception_model' not in locals() or xception_model is None:\n",
        "    print(\"Error: xception_model is not defined or is None. Please run the model building cell first.\")\n",
        "    fine_tuned_model = None\n",
        "else:\n",
        "    fine_tuned_model = fine_tune_model(xception_model, base_layers_unfreeze=50)\n",
        "\n",
        "# Proceed with training only if fine_tuned_model was successfully created\n",
        "if fine_tuned_model is not None:\n",
        "    # Callbacks for fine-tuning\n",
        "    fine_tune_callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=15,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.2,\n",
        "            patience=10,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            '/content/drive/MyDrive/DL Assignment/xception_fine_tuned_best.h5',\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Fine-tuning training\n",
        "    print(\"Starting fine-tuning training...\")\n",
        "    fine_tune_history = fine_tuned_model.fit(\n",
        "        train_generator, # Use train_generator\n",
        "        steps_per_epoch=len(train_generator),\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,  # Use validation_generator here\n",
        "        validation_steps=len(validation_generator), # Use validation_generator here\n",
        "        callbacks=fine_tune_callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    fine_tuned_model.save('/content/drive/MyDrive/DL Assignment/xception_fine_tuned_final.h5')\n",
        "    print(\"Fine-tuned model saved!\")\n",
        "else:\n",
        "    print(\"Fine-tuning model creation failed. Skipping training.\")"
      ],
      "metadata": {
        "id": "8fy9C-9GDgfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot fine-tuning training history\n",
        "def plot_fine_tune_training(history):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot training & validation accuracy\n",
        "    axes[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    axes[0].set_title('Fine-tuning Model Accuracy')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # Plot training & validation loss\n",
        "    axes[1].plot(history.history['loss'], label='Training Loss')\n",
        "    axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
        "    axes[1].set_title('Fine-tuning Model Loss')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_fine_tune_training(fine_tune_history)"
      ],
      "metadata": {
        "id": "FQ2zj6rwDjuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## comparison between initial training and fine-tuning"
      ],
      "metadata": {
        "id": "FtEjSZ6wDnLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare initial training vs fine-tuning\n",
        "def compare_training_histories(initial_history, fine_tune_history):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Accuracy comparison\n",
        "    axes[0, 0].plot(initial_history.history['accuracy'], label='Initial Training', alpha=0.7)\n",
        "    axes[0, 0].plot(initial_history.history['val_accuracy'], label='Initial Validation', alpha=0.7)\n",
        "    axes[0, 0].set_title('Initial Training - Accuracy')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "\n",
        "    axes[0, 1].plot(fine_tune_history.history['accuracy'], label='Fine-tuning Training', alpha=0.7)\n",
        "    axes[0, 1].plot(fine_tune_history.history['val_accuracy'], label='Fine-tuning Validation', alpha=0.7)\n",
        "    axes[0, 1].set_title('Fine-tuning - Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "\n",
        "    # Loss comparison\n",
        "    axes[1, 0].plot(initial_history.history['loss'], label='Initial Training', alpha=0.7)\n",
        "    axes[1, 0].plot(initial_history.history['val_loss'], label='Initial Validation', alpha=0.7)\n",
        "    axes[1, 0].set_title('Initial Training - Loss')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True)\n",
        "\n",
        "    axes[1, 1].plot(fine_tune_history.history['loss'], label='Fine-tuning Training', alpha=0.7)\n",
        "    axes[1, 1].plot(fine_tune_history.history['val_loss'], label='Fine-tuning Validation', alpha=0.7)\n",
        "    axes[1, 1].set_title('Fine-tuning - Loss')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Compare if both histories are available\n",
        "compare_training_histories(history, fine_tune_history)"
      ],
      "metadata": {
        "id": "br3vpaweDpQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluation of fine-tuned model"
      ],
      "metadata": {
        "id": "MzOMOPNLDsVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate fine-tuned model\n",
        "print(\"Evaluating fine-tuned model...\")\n",
        "fine_tune_test_loss, fine_tune_test_accuracy, fine_tune_test_precision, fine_tune_test_recall = fine_tuned_model.evaluate(\n",
        "    test_generator,\n",
        "    steps=len(test_generator),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "fine_tune_test_f1 = 2 * (fine_tune_test_precision * fine_tune_test_recall) / (fine_tune_test_precision + fine_tune_test_recall)\n",
        "\n",
        "print(f\"\\nFine-tuned Model Test Results:\")\n",
        "print(f\"Accuracy: {fine_tune_test_accuracy:.4f}\")\n",
        "print(f\"Precision: {fine_tune_test_precision:.4f}\")\n",
        "print(f\"Recall: {fine_tune_test_recall:.4f}\")\n",
        "print(f\"F1-Score: {fine_tune_test_f1:.4f}\")\n",
        "\n",
        "# Compare with initial model results\n",
        "print(f\"\\nComparison with Initial Model:\")\n",
        "print(f\"Accuracy:  {test_accuracy:.4f} (initial) -> {fine_tune_test_accuracy:.4f} (fine-tuned)\")\n",
        "print(f\"Precision: {test_precision:.4f} (initial) -> {fine_tune_test_precision:.4f} (fine-tuned)\")\n",
        "print(f\"Recall:    {test_recall:.4f} (initial) -> {fine_tune_test_recall:.4f} (fine-tuned)\")\n",
        "print(f\"F1-Score:  {test_f1:.4f} (initial) -> {fine_tune_test_f1:.4f} (fine-tuned)\")"
      ],
      "metadata": {
        "id": "MU3Zla5-Dqmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine-tuning analysis"
      ],
      "metadata": {
        "id": "7djqfPJlD8_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detailed analysis of fine-tuning impact\n",
        "def analyze_fine_tuning_impact():\n",
        "    # Get predictions from both models for comparison\n",
        "    test_generator.reset()\n",
        "    # Use xception_model for initial predictions\n",
        "    initial_preds = xception_model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
        "    test_generator.reset()\n",
        "    fine_tune_preds = fine_tuned_model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
        "\n",
        "    # Convert predictions to binary\n",
        "    initial_binary_preds = (initial_preds > 0.5).astype(int)\n",
        "    fine_tune_binary_preds = (fine_tune_preds > 0.5).astype(int)\n",
        "\n",
        "    # Get true labels\n",
        "    true_labels = test_generator.classes[:len(initial_binary_preds)]\n",
        "\n",
        "    # Calculate confusion matrices\n",
        "    initial_cm = confusion_matrix(true_labels, initial_binary_preds)\n",
        "    fine_tune_cm = confusion_matrix(true_labels, fine_tune_binary_preds)\n",
        "\n",
        "    # Plot comparison\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Initial model confusion matrix\n",
        "    sns.heatmap(initial_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
        "    axes[0].set_title('Initial Model - Confusion Matrix')\n",
        "    axes[0].set_xlabel('Predicted')\n",
        "    axes[0].set_ylabel('Actual')\n",
        "\n",
        "    # Fine-tuned model confusion matrix\n",
        "    sns.heatmap(fine_tune_cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
        "    axes[1].set_title('Fine-tuned Model - Confusion Matrix')\n",
        "    axes[1].set_xlabel('Predicted')\n",
        "    axes[1].set_ylabel('Actual')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print improvement analysis\n",
        "    initial_tn, initial_fp, initial_fn, initial_tp = initial_cm.ravel()\n",
        "    fine_tune_tn, fine_tune_fp, fine_tune_fn, fine_tune_tp = fine_tune_cm.ravel()\n",
        "\n",
        "    print(\"\\nImprovement Analysis:\")\n",
        "    print(f\"True Positives:  {initial_tp} -> {fine_tune_tp} (Change: {fine_tune_tp - initial_tp:+d})\")\n",
        "    print(f\"False Negatives: {initial_fn} -> {fine_tune_fn} (Change: {fine_tune_fn - initial_fn:+d})\")\n",
        "    print(f\"False Positives: {initial_fp} -> {fine_tune_fp} (Change: {fine_tune_fp - initial_fp:+d})\")\n",
        "    print(f\"True Negatives:  {initial_tn} -> {fine_tune_tn} (Change: {fine_tune_tn - initial_tn:+d})\")\n",
        "\n",
        "analyze_fine_tuning_impact()"
      ],
      "metadata": {
        "id": "Rqm-YDJZD_Vi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}